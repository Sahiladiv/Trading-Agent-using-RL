{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from gym.spaces import Box, Discrete\n",
    "from gym import Env\n",
    "import torch.nn as nn\n",
    "from stock_trading_env import StockTradingEnv\n",
    "from ppo_agent import PPO\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS:\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, model, optimizer, epochs=10, gamma=0.99, clip_epsilon=0.2, batch_size=32, lambda_gae=0.95):\n",
    "    for episode in range(epochs):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        dones = []\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            action_probs, _ = model(state_tensor)\n",
    "\n",
    "            print(f\"Action Probabilities: {action_probs.detach().numpy()}\")\n",
    "\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "\n",
    "            print(f\"PPO Chose Action: {action.item()}\")\n",
    "\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            actions.append(action)\n",
    "            dones.append(done)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Compute GAE advantages\n",
    "        values = torch.stack(values).squeeze()\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + (gamma * values[t + 1] if t < len(rewards) - 1 else 0) - values[t]\n",
    "            gae = delta + gamma * lambda_gae * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "        returns = advantages + values.detach()\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        # PPO Update\n",
    "        ratio = torch.exp(log_probs - log_probs.detach())\n",
    "        surrogate1 = ratio * advantages\n",
    "        surrogate2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "        value_loss = (returns - values).pow(2).mean()\n",
    "        \n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Loss: {loss.item()}, Policy Loss: {policy_loss.item()}, Value Loss: {value_loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the trained PPO agent\n",
    "def evaluate_ppo(env, model, episodes=10):\n",
    "    all_rewards = []\n",
    "    portfolio_values = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        portfolio_value = [env.initial_balance]\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            action_probs, _ = model(state_tensor)\n",
    "            action = torch.argmax(action_probs).item()\n",
    "            \n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            portfolio_value.append(env.balance + (env.shares_held * env.stock_data[env.current_step][3]))\n",
    "        \n",
    "        all_rewards.append(total_reward)\n",
    "        portfolio_values.append(portfolio_value)\n",
    "    \n",
    "    return all_rewards, portfolio_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "env = StockTradingEnv()\n",
    "model = PPO(input_dim=9, output_dim=3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "env.render()\n",
    "train_ppo(env, model, optimizer, epochs=500)\n",
    "\n",
    "\n",
    "# Evaluate the agent\n",
    "rewards, portfolio_values = evaluate_ppo(env, model, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.17210114e+01 7.27765906e+01 7.14668048e+01 ... 1.35480400e+08\n",
      "  7.40863770e+01 7.41229780e+01]\n",
      " [7.19413358e+01 7.27717523e+01 7.17839694e+01 ... 1.46322800e+08\n",
      "  7.40863770e+01 7.41229780e+01]\n",
      " [7.11278585e+01 7.26216386e+01 7.08760678e+01 ... 1.18387200e+08\n",
      "  7.40863770e+01 7.41229780e+01]\n",
      " ...\n",
      " [1.28179661e+02 1.29524031e+02 1.24423341e+02 ... 8.54384000e+07\n",
      "  1.31916209e+02 1.41923730e+02]\n",
      " [1.26518978e+02 1.28980357e+02 1.26261971e+02 ... 7.57037000e+07\n",
      "  1.30571840e+02 1.41648888e+02]\n",
      " [1.26934142e+02 1.28456435e+02 1.25965402e+02 ... 7.70342000e+07\n",
      "  1.29922391e+02 1.41378200e+02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "d:\\Machine Learning\\RL project\\Trading Agent\\stock_trading_env.py:36: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Visualization\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# for i, values in enumerate(portfolio_values):\n",
    "#     plt.plot(values, label=f\"Episode {i+1}\")\n",
    "# plt.xlabel(\"Time Steps\")\n",
    "# plt.ylabel(\"Portfolio Value ($)\")\n",
    "# plt.title(\"Portfolio Value Over Time for Different Episodes\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(rewards, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "# plt.xlabel(\"Evaluation Episode\")\n",
    "# plt.ylabel(\"Total Reward\")\n",
    "# plt.title(\"Total Reward per Evaluation Episode\")\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
